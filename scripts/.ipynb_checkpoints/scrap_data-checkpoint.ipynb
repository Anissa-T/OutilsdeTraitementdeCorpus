{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc983e-53b2-4ed8-8bd8-58d2702f2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def lire_urls(chemin_fichier):\n",
    "    \"\"\"Lire les URL à partir d'un fichier texte.\"\"\"\n",
    "    with open(chemin_fichier, 'r') as fichier:\n",
    "        # Retourne une liste des URL en supprimant les lignes vides\n",
    "        return [ligne.strip() for ligne in fichier if ligne.strip()]\n",
    "\n",
    "def scraper_links(url):\n",
    "    \"\"\"Récupérer tous les liens d'une page.\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        # Effectue une requête HTTP pour obtenir le contenu de la page\n",
    "        reponse = requests.get(url, headers=headers)\n",
    "        reponse.raise_for_status()\n",
    "\n",
    "        # Analyse le HTML de la page\n",
    "        soupe = BeautifulSoup(reponse.text, 'html.parser')\n",
    "\n",
    "        # Trouve tous les liens dans la page\n",
    "        liens = soupe.find_all('a')\n",
    "\n",
    "        # Extrait les URL complètes des liens\n",
    "        urls = [lien['href'] for lien in liens]\n",
    "\n",
    "        # Retourne une liste d'URLs\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Échec du scraping pour {url} : {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def scraper_article(url):\n",
    "    \"\"\"Récupérer le titre, le contenu d'un article complet et la description.\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        # Effectue une requête HTTP pour obtenir le contenu de la page\n",
    "        reponse = requests.get(url, headers=headers)\n",
    "        reponse.raise_for_status()\n",
    "\n",
    "        # Analyse le HTML de la page\n",
    "        soupe = BeautifulSoup(reponse.text, 'html.parser')\n",
    "\n",
    "        # Trouve le titre de l'article\n",
    "        balise_titre = soupe.find('title')\n",
    "        titre = balise_titre.get_text(strip=True) if balise_titre else 'Titre non trouvé'\n",
    "\n",
    "        # Trouve le contenu de l'article, en excluant la description\n",
    "        balise_article = soupe.find('article')\n",
    "        if balise_article:\n",
    "            # Supprime les éléments de description potentiels du contenu de l'article\n",
    "            for meta in balise_article.find_all('meta', attrs={'name': 'description'}):\n",
    "                meta.decompose()\n",
    "\n",
    "            # Collecte tous les paragraphes et les balises de titre dans le tag article\n",
    "            contenu = '\\n'.join([p.get_text(strip=True) for p in balise_article.find_all(['p', 'h2', 'h3', 'h4'])])\n",
    "        else:\n",
    "            contenu = 'Contenu de l\\'article non trouvé'\n",
    "\n",
    "        # Combine le titre et le contenu en un article complet\n",
    "        article_complet = f\"{titre}\\n\\n{contenu}\"\n",
    "\n",
    "        # Extrait la description séparément\n",
    "        balise_description = soupe.find('meta', attrs={'name': 'description'})\n",
    "        description = balise_description['content'] if balise_description else 'Aucune description trouvée'\n",
    "\n",
    "        # Retourne un dictionnaire contenant l'URL, l'article complet et la description\n",
    "        return {\"id\": url, \"article\": article_complet, \"description\": description}\n",
    "    except Exception as e:\n",
    "        print(f\"Échec du scraping pour {url} : {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def ecrire_json(donnees, fichier_json):\n",
    "    \"\"\"Écrire la liste de dictionnaires dans un fichier JSON.\"\"\"\n",
    "    repertoire_sortie = 'donnees_sortie'\n",
    "    # Crée le répertoire de sortie s'il n'existe pas\n",
    "    if not os.path.exists(repertoire_sortie):\n",
    "        os.makedirs(repertoire_sortie)\n",
    "\n",
    "    # Ouvre le fichier JSON en mode écriture\n",
    "    with open(os.path.join(repertoire_sortie, fichier_json), 'w', encoding='utf-8') as fichier:\n",
    "        json.dump(donnees, fichier, ensure_ascii=False, indent=4)\n",
    "\n",
    "def principal():\n",
    "    \"\"\"Fonction principale pour scraper les articles et enregistrer les données dans un fichier JSON.\"\"\"\n",
    "    chemin_fichier = '../data/url_leparisien.txt'  # Chemin vers le fichier contenant les URL\n",
    "    fichier_json = 'donnees_scrapees.json'  # Nom du fichier JSON de sortie\n",
    "\n",
    "    urls = lire_urls(chemin_fichier)  # Lecture des URL depuis le fichier\n",
    "    articles = []  # Liste pour stocker les articles\n",
    "    failed_urls = []  # Liste pour stocker les URL échouées\n",
    "    valid_urls = 0  # Compteur pour les URLs valides\n",
    "\n",
    "    # Scraping de chaque URL\n",
    "    for url in urls:\n",
    "        if valid_urls >= 500:  # Arrête le scraping une fois que nous avons 500 URLs valides\n",
    "            break\n",
    "        links = scraper_links(url)  # Scraping des liens\n",
    "        for link in links:\n",
    "            article = scraper_article(link)  # Scraping de l'article\n",
    "            if article:\n",
    "                articles.append(article)  # Ajoute l'article à la liste\n",
    "                valid_urls += 1  # Incrémente le compteur de URLs valides\n",
    "            else:\n",
    "                failed_urls.append(link)  # Ajoute l'URL à la liste des échecs\n",
    "            time.sleep(0.1)  # Pause pour éviter de surcharger le serveur\n",
    "\n",
    "    # Réessayer les URL échouées\n",
    "    for url in failed_urls:\n",
    "        if valid_urls >= 500:  # Arrête le scraping une fois que nous avons 500 URLs valides\n",
    "            break\n",
    "        article = scraper_article(url)  # Scraping de l'article\n",
    "        if article:\n",
    "            articles.append(article)  # Ajoute l'article à la liste\n",
    "            failed_urls.remove(url)  # Retire l'URL de la liste des échecs\n",
    "            valid_urls += 1  # Incrémente le compteur de URLs valides\n",
    "        time.sleep(0.1)  # Pause pour éviter de surcharger le serveur\n",
    "\n",
    "    ecrire_json(articles, fichier_json)  # Écriture des articles dans un fichier JSON\n",
    "\n",
    "    # Affichage des résultats\n",
    "    with open(os.path.join('donnees_sortie', fichier_json), 'r', encoding='utf-8') as fichier:\n",
    "        donnees = json.load(fichier)\n",
    "        print(json.dumps(donnees, ensure_ascii=False, indent=4))\n",
    "\n",
    "    # Affichage des URL échouées s'il y en a\n",
    "    if failed_urls:\n",
    "        print(\"Les URL suivantes ont échoué après deux tentatives :\")\n",
    "        for url in failed_urls:\n",
    "            print(url)\n",
    "\n",
    "# Exécution de la fonction principale si le script est exécuté directement\n",
    "if __name__ == \"__main__\":\n",
    "    principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08a82e-d90f-45d1-a19f-1f845f852cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
