# 🌻 Outils de Traitement de Corpus 🌻 
Anissa Thezenas 👩🏾‍💻

1. Je souhaites réaliser la tâche suivante : Natural Language Processing : Summarization  📄
2. un corpus qui répond à cette tâche : link 🔗 : https://huggingface.co/datasets/databricks/databricks-dolly-15k
3. à quel type de prédiction peut servir ce corpus : ce corpus peut servir à faire des résumers de textes plus ou moins long, permettre à un model de faire des paraphrases. Extraction d'informations, 
4. à quel modèle il a servi : databricks/dolly-v2-3b ou encore TheBloke/Mythalion-13B-AWQ 🤖
5. Apprenez moi des choses sur un corpus :  le corpus se nomme Dolly comme le premier mouton cloné ? 🐑 De plus le corpus est composé de 15 011 lignes 

Ajustements : Le dataset Databricks Dolly 15k est une collection conçue spécialement pour le réglage des instructions des grands modèles de langage. Il comprend plus de 15 000 paires de consignes et réponses générées par plus de 5 000 employés de Databricks pendant les mois de mars et avril 2023. Ce dataset vise à développer des modèles capables d'imiter les capacités conversationnelles de systèmes comme ChatGPT.

Ce dataset couvre diverses catégories comportementales telles que définies dans le document InstructGPT, incluant le brainstorming, la classification, les questions-réponses fermées (QA), la génération, l'extraction d'informations, les questions-réponses ouvertes, et la résumé. Les contributeurs, tous employés de Databricks, ont été guidés pour créer ces paires sans utiliser d'informations provenant du web, à l'exception de Wikipédia pour certaines tâches spécifiques, et il leur a été recommandé de ne pas utiliser d'IA générative pour formuler les contenus, garantissant ainsi que les réponses sont véritablement générées par des humains.

Le dataset Databricks Dolly 15k est open source, disponible sous la licence Creative Commons Attribution-ShareAlike 3.0 Unported, ce qui permet son utilisation à des fins académiques et commerciales. Cela inclut des tâches telles que la formation de modèles de langage, la génération de données synthétiques et l'augmentation de données.